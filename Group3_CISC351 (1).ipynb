{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CISC 351 Group 3 - Google Analytics Customer Revenue Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": false
   },
   "source": [
    "In order to perform this regression task, the overall process followed is shown in the contents below. This process started off with the importation of relevant libraries and a prelimary assesment of the data. Afterwards, tasks such as exploratory analysis and the preprocessing of data was conducted. Lastly, the 2 baseline models were built, implemented, and evaluated. Detailed discussion can be found in the report handed in.\n",
    "\n",
    "1. [Preliminary Data Assesment](#1)\n",
    "2. [Preprocessing Data](#2)\n",
    "    1. [JSon Parser Function](#2.1)\n",
    "    2. [Function to load in a Random 1% Sample of Data](#2.2)\n",
    "    3. [Constant Columns and Missing Values Analysis](#2.3)   \n",
    "    4. [Feature Engineering](#2.4)    \n",
    "3. [Building 2 Baseline Models](#3)\n",
    "    1. [Separate Features, Target,and Construct Validation Set](#3.1)   \n",
    "    2. [LGBM](#3.2)\n",
    "    3. [XGBoost](#3.3)\n",
    "    4. [Summary and Comparison of the Two Models](#3.4)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## Preliminary Data Assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With any machine learning task, the team started off with importing the necessary libraries to perform the regression task. A portion of the data was read in and assessed. In addition, the sizes of the training and test sets was analyzed. This analysis found two difficulties in dealing with the dataset. First, the data was parsed in JSon queries. Second, the dataset size was very large, with the training data having over a million rows and the testing data having over several hundred thousand rows. Discussions on how the team dealt with this issue take place in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4ef7cad1f68b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import datetime as dt\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import time\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "4f8c7056484b0ddc504c588b27292f24a93c1451"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/train_v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-30a72e7e800e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Determine Sizes of Training and Testing Sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtraining_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/train_v2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtesting_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/test_v2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Size of training set is: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rows. Size of testing set is: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rows.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/train_v2.csv'"
     ]
    }
   ],
   "source": [
    "#Determine Sizes of Training and Testing Sets\n",
    "training_size = int(sum(1 for line in open('../input/train_v2.csv')))\n",
    "testing_size = int(sum(1 for line in open('../input/test_v2.csv')))\n",
    "print('Size of training set is: ', training_size, 'rows. Size of testing set is: ', testing_size, 'rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b3ec34eca8a79337bf45ae9ac597727ed6f4f1e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load portion of training data to assess the data\n",
    "JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "df2 = pd.read_csv('../input/train_v2.csv', nrows=1000)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "### JSon Parser Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above, one of the first issues encountered with the dataset was that it was parsed in JSon queries. After trial and error, the the following function was defined succesfully to parse the JSon queries in Pandas dataframes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca110b71b299e13fb9859ba41b2ed4e320d1b36c"
   },
   "outputs": [],
   "source": [
    "#Define function to parse the JSON entries\n",
    "def Parser (df):\n",
    "    'This function takes a Pandas dataframe as an input, parses the JSON columns in the dataframe, and produces a parsed Pandas dataframe'\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.2\"></a>\n",
    "### Function to load in a Random 1% Sample of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the preliminary data assesment, there were two approaches taken to dealing with the massive size of the dataset. The first approach taken was to load and preprocess the data in batches with a function called the dataPipeline function that can be found commented out further down in the code. The idea with this function was to load the data in 1% batches, preprocess each batch thereby reducing the dataframe size signficantly enough that once all the 1% batches were concatenated together, the dataframe would be able to fit into memory. The function ran and produced an dataframe output, but the result was not to expectation. Due to a time constraint on the project, the team decided to use the function defined below called load_DF that generates a random 1% sample of the training and test set. This function is initialized with a random generator with random.seed(25) to produce the same random dataframe everytime. As this function produces a completely random dataframe, the dataframe distribution should be representative of the entire dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "749dae84b9f202c9d1bec45cc7a31e1683fb0ab3"
   },
   "outputs": [],
   "source": [
    "# Define function to randomly load in 1% of train/test data\n",
    "def load_DF(filename):\n",
    "    'This function takes a csv file as an input, and outputs 1% of that files rows randomly selected '\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource'] #JSon Columns in the training set\n",
    "    num_lines = int(sum(1 for line in open(filename))) # Read number of file lines in the training dataset\n",
    "    print('The number of rows in the file is ',num_lines) \n",
    "    n = int(num_lines-1) #number of records in file (excludes header)\n",
    "    s = int(0.01*n) #Desired sample of 10 Percent of Dataset\n",
    "    random.seed(25)\n",
    "    skip = sorted(random.sample(range(1,n+1),n-s)) #the 0-indexed header will not be included in the skip list\n",
    "    DF=pd.read_csv(filename, converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                        dtype={'fullVisitorId': 'str'},skiprows=skip)\n",
    "    \n",
    "    DF = Parser(DF)\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5aecf331192c206658bc1227e0ab0d8d9392f97d"
   },
   "outputs": [],
   "source": [
    "#Unparse Randomly chosen training data for feature engineering\n",
    "trainingData = load_DF('../input/train_v2.csv')\n",
    "#Unparse Randomly chosen testing data for feature engineering\n",
    "testingData = load_DF('../input/test_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5cbd22b749d3916e3fd9a75bd59c031cd7cd0a2"
   },
   "outputs": [],
   "source": [
    "print(trainingData.shape)\n",
    "trainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd8d9837f0a332d34c3e06252ea9372bf8b8c71d"
   },
   "outputs": [],
   "source": [
    "print(testingData.shape)\n",
    "testingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata[\"totals.transactionRevenue\"] = trainingdata[\"totals.transactionRevenue\"].astype('float')\n",
    "gdf = trainingdata.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(gdf.shape[0]), np.sort(np.log1p(gdf[\"totals.transactionRevenue\"].values)))\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('TransactionRevenue', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.3\"></a>\n",
    "### Constant Columns & Missing Values Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading in the random 1% sample of the training and testing data, the team explored the data and determined the number of columns with constant values and performed a missing values analysis on the two dataframes with the missing_values function. This analysis determined that both the training and testing datasets had a large number of columns with constant values and a large amount of missing values. These will be discussed further in the analysis of the Feature Engineering function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining Columns with Constant Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa44da9e4ae749df3e2a99d5cf15a90370da862a"
   },
   "outputs": [],
   "source": [
    "# Determine the number of columns in dataset with constant values \n",
    "columns_constant = [i for i in trainingData.columns if trainingData[i].nunique(dropna=False)==1 ]\n",
    "columns_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the Missing Values in the Training and Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0067bfc9c29b8396381555039559fe0dc0a65781"
   },
   "outputs": [],
   "source": [
    "# Analyze the missing values in the dataset \n",
    "def missing_values(data):\n",
    "    'Input: Dataframe, Output: Dataframe. Function analyzes the missing values in a dataframe and returns the percentage missing.'\n",
    "    #Count up all the null values \n",
    "    missing = data.isnull().sum().sort_values(ascending = False) \n",
    "    # Get Count of total values  \n",
    "    Count= data.count().sort_values(ascending=False)\n",
    "    #Express Missing values as a percent of total values \n",
    "    Percent = (missing/(Count+missing)*100).sort_values(ascending=False)\n",
    "    df = pd.concat([missing, Count, Percent], axis=1, keys=['Missing', 'Count', 'Percent'])\n",
    "    print (df[~(df['Percent'] == 0)]) # Returning values of nulls different of 0    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e82fce0648fe6f81d9b0b5fe212f04d209d4442c"
   },
   "outputs": [],
   "source": [
    "missing_values(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0c00d3bc75b64b0d7c61c6d323f1cbd60b128e5"
   },
   "outputs": [],
   "source": [
    "missing_values(testingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.4\"></a>\n",
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature engineering function accomplishes numerous tasks necessary prior to the machine learning models. First, when analyzing the dataframe, it was determined that there were multiple features that were similar to the target feature totals.transactionRevenue such as totals.totalTransactionRevenue and totals.transactions. Including these in our analysis would likely have skewed the final results of the model as they contain very similar values to the target. Therefore, these features were removed. Other features deemed to possess no analytical value such as the visitID and hits were removed. Included in this category are features that have over 95% missing values such as trafficSource.adwordsClickInfo.adNetworkType. Values could have been imputed for the missing values in this columns, but when over 95% of the values in the column are missing it likely would not have yielded ideal results. \n",
    "\n",
    "Other columns with missing values were left unaddressed, as both of the models being employed in this problem are able to deal with missing values with strong in-built methods. Next, the categoric features were encoded, and the numeric columns were converted to float values. Additionally, missing values for the target totals.transactionRevenue were imputed with zero. This is reasonable, as missing values in these columns likely indicates that no transaction took place. Lastly, the date feature was converted into a standard date format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2266fb9edd72541fd684879ba8f5f09654609a0"
   },
   "outputs": [],
   "source": [
    "def featureEngineering(df):\n",
    "    'Function requires a dataframe as an input, and will output a feature Engineered dataframe'\n",
    "    #Calculate number of constant columns that can be dropped\n",
    "    columns_constant = [i for i in df.columns if df[i].nunique(dropna=False)==1 ]\n",
    "    # Drop features that are very similar to our target feature or will have no analytical value\n",
    "    similarToFeature = ['totals.totalTransactionRevenue'] + ['totals.transactions']\n",
    "    noAnalyticalValue =  ['visitId']+['customDimensions']+['hits']+['trafficSource.adwordsClickInfo.adNetworkType']+['trafficSource.adwordsClickInfo.isVideoAd']+['trafficSource.adwordsClickInfo.page']+['trafficSource.adwordsClickInfo.slot']\n",
    "    tot = similarToFeature + noAnalyticalValue\n",
    "    if 'totals.totalTransactionRevenue' and 'totals.transactions' and 'visitId' and 'fullVisitorId' and 'customDimensions' and 'hits' and 'trafficSource.adwordsClickInfo.adNetworkType' and 'trafficSource.adwordsClickInfo.isVideoAd'and 'trafficSource.adwordsClickInfo.page' and 'trafficSource.adwordsClickInfo.slot'in df.columns:\n",
    "        columns_drop = columns_constant + tot\n",
    "        #Drop constant columns \n",
    "        df= df.drop(columns_drop, axis=1)\n",
    "    else:\n",
    "        df= df.drop(columns_constant, axis=1)\n",
    "        \n",
    "    #Encode the categoric features to numeric \n",
    "    categoric_features = [\"channelGrouping\", \"device.browser\", \n",
    "            \"device.deviceCategory\", \"device.operatingSystem\", \n",
    "            \"geoNetwork.city\", \"geoNetwork.continent\", \n",
    "            \"geoNetwork.country\", \"geoNetwork.metro\",\n",
    "            \"geoNetwork.networkDomain\", \"geoNetwork.region\", \n",
    "            \"geoNetwork.subContinent\", \"trafficSource.adContent\", \n",
    "            \"trafficSource.adwordsClickInfo.adNetworkType\", \n",
    "            \"trafficSource.adwordsClickInfo.gclId\", \n",
    "            \"trafficSource.adwordsClickInfo.page\", \n",
    "            \"trafficSource.adwordsClickInfo.slot\", \"trafficSource.campaign\",\n",
    "            \"trafficSource.keyword\", \"trafficSource.medium\", \n",
    "            \"trafficSource.referralPath\", \"trafficSource.source\",\n",
    "            'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.isTrueDirect', 'device.isMobile']\n",
    "    for i in categoric_features:\n",
    "        if i in df.columns:\n",
    "            encode = sklearn.preprocessing.LabelEncoder()\n",
    "            encode.fit(df[i].values.astype('str'))\n",
    "            df[i] = encode.transform(list(df[i].values.astype('str')))\n",
    "    \n",
    "    #Convert numeric features to float values\n",
    "    numeric_features = [\"totals.hits\", \"totals.pageviews\",'totals.transactionRevenue',\"visitNumber\", \"visitStartTime\", 'totals.bounces',  'totals.newVisits','totals.sessionQualityDim', 'totals.timeOnSite']\n",
    "    for i in numeric_features:\n",
    "        df[i] = df[i].astype(float)\n",
    "    \n",
    "    #Encode Zeroes for nan values in totals.bounces & totals.transactionRevenues\n",
    "    df['totals.transactionRevenue'].fillna(0,inplace=True)\n",
    "    \n",
    "    #Convert date to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'], format = \"%Y%m%d\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b4699c09da136c3af47aa8eab59c81a3e8b1696b"
   },
   "outputs": [],
   "source": [
    "#FeatureEngineer training and testing data \n",
    "trainingData = featureEngineering(trainingData)\n",
    "testingData = featureEngineering(testingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempted Reading in Batches\n",
    "# Function runs and produces an output but not as intended \n",
    "# def dataPipeline(filename):\n",
    "#     'dataPipeline is a function that takes a csv as an input, and returns a fully pre-processed Pandas dataframe ready for'\n",
    "#     'ML Algorithms'\n",
    "#     #Start Time of Function\n",
    "#     start_time = dt.datetime.now()\n",
    "#     print(\"Started at \", start_time)\n",
    "#     # Read number of file lines in the dataset\n",
    "#     num_lines = int(sum(1 for line in open(filename))) \n",
    "#     # Process in chunk sizes of 1%\n",
    "#     batchSize = int(0.01*num_lines)\n",
    "#     # Initialize a List to hold all of the processed, chunked dataframes\n",
    "#     result = []\n",
    "#     #For Parser\n",
    "#     JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "#     print('Entering into processing the csv file by batches')\n",
    "#     for batch in pd.read_csv(filename, chunksize=batchSize,\n",
    "#                      converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "#                      dtype={'fullVisitorId': 'str'}):\n",
    "#         parsed = Parser(batch)\n",
    "#         processed = featureEngineering(parsed)\n",
    "#         result.append(processed)\n",
    "#     Output = pd.concat(result)\n",
    "#     #Time it took for function to finish\n",
    "#     end_time = dt.datetime.now()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print (\"Preprocessed \", filename, \"completely. Elapsed time: \", elapsed_time)\n",
    "#     return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "089616ce9679f16c49696249f2a760e10224b480"
   },
   "outputs": [],
   "source": [
    "print(trainingData.shape)\n",
    "trainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b00ba61a56bc25dfdf1f85817a146d7c7934a33"
   },
   "outputs": [],
   "source": [
    "print(testingData.shape)\n",
    "testingData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## Building 2 Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two baseline models chosen to be used for this regression task were the LGBM and the XGB models. The XGB model was chosen as it has proven to consistently perform well on machine learning challenges on Kaggle. The LGBM is a newer model that is less proven, but is stated to run multiple times faster than the XGB and achieve the same accuracies. The two models will be compared in this section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "### Seperate Features, Target, and Construct Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step taken in building the two models was the separation of the training set into a training and validation set based on an 80/20 split on the dat feature in the training set between the two. This split between the training and validation set is a typical sized split that should help prevent overtraining of the models being built. Afterwards, the three datasets were split into their respective targets and features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "349497f1d6d4ff558288b554feccc6fed1d7dcf0"
   },
   "outputs": [],
   "source": [
    "#Split training set for validation set \n",
    "train_df = trainingData[trainingData['date']> dt.date(2016,11,30)]\n",
    "validation_df = trainingData[trainingData['date']<=dt.date(2016,11,30)]\n",
    "\n",
    "features = [] \n",
    "label = []\n",
    "for i in trainingData.columns:\n",
    "    if i != 'totals.transactionRevenue' and i!='date' and i!= 'fullVisitorId' and i!= 'visitStartTime':\n",
    "        features.append(i)\n",
    "    elif i == 'totals.transactionRevenue':\n",
    "        label.append(i)\n",
    "#Target\n",
    "train_y = np.log1p(train_df[label])\n",
    "validation_y = np.log1p(validation_df[label])\n",
    "test_y = np.log1p(testingData[label])\n",
    "#Features\n",
    "train_x = train_df[features]\n",
    "validation_x = validation_df[features]\n",
    "test_x = testingData[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa982ce8c8b85cb02c57928678fe4059c3722e6d"
   },
   "outputs": [],
   "source": [
    "print('training dataframe shape:' , train_df.shape)\n",
    "print('validation dataframe shape:' , validation_df.shape)\n",
    "print('training features shape:' , train_x.shape)\n",
    "print('validation features shape:' , validation_x.shape)\n",
    "print('training label shape:' , train_y.shape)\n",
    "print('validation label shape:' , validation_y.shape)\n",
    "print('testing features shape:' , test_x.shape)\n",
    "print('testing label shape:' , test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "\n",
    "### LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step completed in constructing the LGBM model was hyperparameter tuning conducted using the the training set with the RandomizedSearchCV. The RandomizedSearchCV was used for hyperparameter tuning because it produces reasonable results in a very flexible and efficient manner. During hyperparameter tuning, this model ran extremely fast, being able to fit 3 cross validation folds with 100 iterations totalling 300 possible fits of hyperparameters. An important note here is that since early stopping is being used to optimize the number of estimators, one of the most important parameters in the model, n_estimators was set to an upper maximum bound. The final hyperparameters, other than n_estimators, can be found in the output below. The final tuned parameters were saved and can be seen in the dictionary \"SavedLGBMParams\" in the interest of efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a21e1f2475f595dffa803d2af7cc7bb802f96a7d"
   },
   "outputs": [],
   "source": [
    "# Prior to early stopping, determine best hyperparameters for most important parameters in \n",
    "#LGBM models. n_estimators, one of the most important parameters, is determined by \n",
    "# early stopping, so a maximum limit is set here \n",
    "params_LGB ={'num_leaves': sp_randint(6, 50), \n",
    "             \"learning_rate\": sp_uniform(0.05, 0.4),\n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6)}\n",
    "\n",
    "LGBReg = lgb.LGBMRegressor(n_estimators=1500, random_state=40, silent = True, n_jobs=4,max_depth =-1)\n",
    "LGBHP_opt = RandomizedSearchCV(\n",
    "    estimator=LGBReg, param_distributions=params_LGB, \n",
    "    n_iter=100,\n",
    "    cv=3,scoring='neg_mean_squared_error',random_state=40, refit=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "611bcee3128673455b3732df95e0ede79b1f3d64"
   },
   "outputs": [],
   "source": [
    "LGBHP_opt.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a3292329a18bef1059d9f06768e9c457483ed3d"
   },
   "outputs": [],
   "source": [
    "FinalLGBParams = LGBHP_opt.best_params_\n",
    "FinalLGBParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdditionalParams = {'objective': 'regression', 'metric':'rmse', 'verbosity':-1}\n",
    "for i in AdditionalParams:\n",
    "    FinalLGBParams[i] = AdditionalParams[i]\n",
    "FinalLGBParams\n",
    "print(FinalLGBParams)\n",
    "#Save the Optimum LGB Params to increase efficiency of not having to run each time to obtain them\n",
    "SavedLGBParams = {'colsample_bytree': 0.8539721005885168,\n",
    " 'learning_rate': 0.05722266438555726,\n",
    " 'min_child_samples': 453,\n",
    " 'min_child_weight': 1,\n",
    " 'num_leaves': 35,\n",
    " 'subsample': 0.6218338710274947,\n",
    " 'objective': 'regression',\n",
    " 'metric': 'rmse',\n",
    " 'verbosity': -1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Run LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function defined to build the LGBM is shown below. This function utilizes early stopping to optimize the n_estimators in the model utilizing the training set and the validation set. This is an important step, as early stopping will help mitigate any overfitting issues that could arise when building this model. After the model was built, predictions were obtained using the optimum model as determined by early stopping. These predictions were returned in the function output, along with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b94bfd946b13329f6cfacebac0426d32593541f"
   },
   "outputs": [],
   "source": [
    "#Define function to use Early Stopping LightGBM model with parameters determined from RandomizedSearchCV\n",
    "def LGBM(train_x, train_y, val_x, val_y, test_x, test_y, LGBParams):\n",
    "    lgbtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    lgbval = lgb.Dataset(val_x, val_y)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = lgb.train(LGBParams, lgbtrain, 1000, valid_sets = [lgbval], early_stopping_rounds=100, verbose_eval=100)\n",
    "    end_time = time.time()\n",
    "    print(end_time-start_time)\n",
    "    \n",
    "    predictions_test= model.predict(test_x, num_iteration = model.best_iteration)\n",
    "    predictions_validation = model.predict(val_x, num_iteration= model.best_iteration)\n",
    "    \n",
    "    return predictions_test, predictions_validation, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f694e8c7b3c033816639599418597c438057b65c"
   },
   "outputs": [],
   "source": [
    "# Run LGBM\n",
    "predictions_test, predictions_validation, modellgb = LGBM(train_x, train_y, validation_x, validation_y, test_x, test_y,FinalLGBParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant regression evaluation metrics were then obtained for the model for both the validation data set and testing data set. In summary, the performance of the model is slightly worse on the testing dataset for all metrics considered. This indicates that the model doesn't have any major issues generalizing, and overfitting has been mostly avoided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Set\n",
    "MSE_LGB_test = sklearn.metrics.mean_squared_error(test_y,predictions_test)\n",
    "MAE_LGB_test= sklearn.metrics.mean_absolute_error(test_y, predictions_test)\n",
    "RMSE_LGB_test = np.sqrt(MSE_LGB_test)\n",
    "#Validation Set\n",
    "MSE_LGB_val = sklearn.metrics.mean_squared_error(validation_y, predictions_validation)\n",
    "MAE_LGB_val = sklearn.metrics.mean_absolute_error(validation_y, predictions_validation)\n",
    "RMSE_LGB_val = np.sqrt(MSE_LGB_val)\n",
    "print('Mean square Error for the LGBM testing set: {}, Mean absolute error for the LGBM testing set: {}, Root mean square error for the LGBM testing set: {}'.format(MSE_LGB_test, MAE_LGB_test, RMSE_LGB_test))\n",
    "\n",
    "print('Mean square Error for the LGBM validation set: {}, Mean absolute error for the LGBM validation set: {}, Root mean square error for the LGBM validation set: {}'.format(MSE_LGB_val, MAE_LGB_val, RMSE_LGB_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, feature importances were obtained for the LGBM. The top 3 features as determined by the LGBM are totals.timeOnSite, geoNetwork.networkDomain, and totals.sessionQualityDim. This results seem to be reasonably intuitive after the exploratory analysis that was conducted on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1843a03f6d48cac53253a4c54f3c7d4e72627055"
   },
   "outputs": [],
   "source": [
    "#Obtain and plot figure importance for the LGBM\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "lgb.plot_importance(modellgb, max_num_features=50, height=0.8, ax=ax)\n",
    "ax.grid(False)\n",
    "plt.title(\"LightGBM - Feature Importance\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3\"></a>\n",
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the LGBM, the first step completed for the XGBoost model was hyperparameter tuning with the RandomizedSearchCV. To reiterate what was previously stated, this method allows for very flexible and efficient hyperparameter tuning compared to other methods. As early stopping was again going to be used with the training and validation datasets to determine the n_estimators, the value was set to a high bound of 2500 here. The difference between the two models really showed in this step, as originally 3 cross validation folds with 100 iterations each was used here similar to the LGBM. This took way to long a time to execute. Instead, the cross validation folds was reduced to 2 with 50 iterations in each fold. This solution still took much longer to execute than the LGBM's hyperparameter tuning, at a run time of approximately 29 minutes. The tuned hyperparameters for the XGBoost model were saved in a dictionary called \"SavedXGBParams\" in the interest of efficiency, and can be seen there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "093062995775635960ca95c3ce99704a8ad3062f"
   },
   "outputs": [],
   "source": [
    "# Prior to early stopping, determine best hyperparameters for most important parameters in \n",
    "#XGB model. n_estimators, one of the most important parameters, is determined by \n",
    "# early stopping, so a maximum limit is set here \n",
    "params_XGB= {  \n",
    "    \"learning_rate\": sp_uniform(0.05, 0.4),\n",
    "    \"colsample_bytree\": sp_uniform(loc=0.4, scale=0.6),\n",
    "    'max_depth': sp_randint(3, 40),\n",
    "    \"subsample\": sp_uniform(loc=0.2, scale=0.8),\n",
    "    \"gamma\": sp_uniform(0, 10),\n",
    "    'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "    \"min_child_weight\": [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "}\n",
    "\n",
    "XGBReg = xgb.XGBRegressor(n_estimators=2500, random_state=40, silent = True, n_jobs=4)\n",
    "XGBHP_opt = RandomizedSearchCV(\n",
    "    estimator=XGBReg, param_distributions=params_XGB, \n",
    "    n_iter=50,cv=2,scoring='neg_mean_squared_error',random_state=40, refit=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aee3437904756cd91152e2ee6b3e5f50175e0355"
   },
   "outputs": [],
   "source": [
    "XGBHP_opt.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalXGBParams = XGBHP_opt.best_params_\n",
    "FinalXGBParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdditionalParams = {'objective': 'reg:linear', 'eval_metric':'rmse', 'silent':True, 'random_state':40}\n",
    "for i in AdditionalParams:\n",
    "    FinalXGBParams[i] = AdditionalParams[i]\n",
    "FinalXGBParams\n",
    "#Save the Optimum XGB Params to increase efficiency of not having to run each time to obtain them\n",
    "SavedXGBParams = {'colsample_bytree': 0.5454951393008803,\n",
    " 'gamma': 4.834820481469037,\n",
    " 'learning_rate': 0.050158252068833425,\n",
    " 'max_depth': 36,\n",
    " 'min_child_weight': 1,\n",
    " 'reg_alpha': 100,\n",
    " 'subsample': 0.9278827742521691,\n",
    " 'objective': 'reg:linear',\n",
    " 'eval_metric': 'rmse',\n",
    " 'silent': True,\n",
    " 'random_state': 40}\n",
    "SavedXGBParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Run XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function was used to run the XGBoost model on the dataset. This function utilizes early stopping with the training and validation dataset to find the optimum number of n_estimators to avoid overfitting problems. After the model is built with early stopping, predictions are obtained using the validation set and testing dataset. The function returns the final model and the predictions on both of the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a53cc238fe0c8837c40fc265410c68e29d58e227"
   },
   "outputs": [],
   "source": [
    "#Define function to run XGBoost model\n",
    "def XGB(train_x, train_y, val_x, val_y, test_x, test_y, paramsXGB):    \n",
    "    xgb_train_dataMatrix = xgb.DMatrix(train_x, label = train_y)\n",
    "    xgb_val_dataMatrix = xgb.DMatrix(val_x, label = val_y)\n",
    "    xgb_test_dataMatrix = xgb.DMatrix(test_x, label= test_y)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = xgb.train(paramsXGB, xgb_train_dataMatrix, \n",
    "                      num_boost_round=2000, \n",
    "                      evals= [(xgb_train_dataMatrix, 'train'), (xgb_val_dataMatrix, 'valid')],\n",
    "                      early_stopping_rounds=100, \n",
    "                      verbose_eval=100\n",
    "                     )    \n",
    "    end_time = time.time()\n",
    "    print(end_time-start_time)\n",
    "    \n",
    "    y_pred_val = model.predict(xgb_val_dataMatrix, ntree_limit=model.best_ntree_limit)\n",
    "    y_pred_test = model.predict(xgb_test_dataMatrix, ntree_limit=model.best_ntree_limit)\n",
    "    return y_pred_test, y_pred_val, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "41c7c54b15d1d2c082f720769c39ae14ddc11cf8"
   },
   "outputs": [],
   "source": [
    "#Run XGBM\n",
    "predictions_testingData, predictions_validationData, model = XGB(train_x, train_y, validation_x, validation_y, test_x, test_y,SavedXGBParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics XGBoost generated on the Testing Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant regression evaluation metrics were obtained on both the validation and testing dataset. Similar to the LGBM, the model performs slightly worse on the testing set compared to the validation set. This indicates that the XGBoost model does not have major issues generalizing, and overfitting has been mostly avoided. The models will be compared in the section titled \"Summary and Comparison of the Two Models\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Set\n",
    "MSE_XGB_testing = sklearn.metrics.mean_squared_error(test_y,predictions_testingData)\n",
    "MAE_XGB_testing= sklearn.metrics.mean_absolute_error(test_y, predictions_testingData)\n",
    "RMSE_XGB_testing = np.sqrt(MSE_XGB_testing)\n",
    "\n",
    "#Validation Set\n",
    "MSE_XGB_validation = sklearn.metrics.mean_squared_error(validation_y, predictions_validationData)\n",
    "MAE_XGB_validation = sklearn.metrics.mean_absolute_error(validation_y, predictions_validationData)\n",
    "RMSE_XGB_validation = np.sqrt(MSE_XGB_validation)\n",
    "print('Mean square Error for the XGB testing set: {}, Mean absolute error for the XGB testing set: {}, Root mean square error for the XGB testing set: {}'.format(MSE_XGB_testing, MAE_XGB_testing, RMSE_XGB_testing))\n",
    "\n",
    "print('Mean square Error for the XGB validation set: {}, Mean absolute error for the XGB validation set: {}, Root mean square error for the XGB validation set: {}'.format(MSE_XGB_validation, MAE_XGB_validation, RMSE_XGB_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importances as reported by the XGBoost model are shown in the figure below. The XGBoost reports that total.sessionQualityDim, totals.hits, and totals.timeOnSite are the three most important features for this prediction. These results also seem to be fairly intuitive after the exploratory analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c1e918fd5159eddb286b337e0758219b12f9a45"
   },
   "outputs": [],
   "source": [
    "#Obtain and plot figure importance for the LGBM\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "ax.grid(False)\n",
    "plt.title(\"XGB - Feature Importance\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.4\"></a>\n",
    "### Summary and Comparison of the Two Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, the LightGBM showcased a much faster training time than the XGBoost model. This is best exemplified through the hyperparameter tuning that took place, as the LightGBM model was able to fit 3 cross validation folds with 100 possible combinations in each fold in approximately 8 minutes, while the XGBoost model fit 2 cross validation folds with 50 possible combinations in each fold in approximately 29 minutes. Next, the LightGBM model had slightly better accuracy on MSE and RMSE on both the validation set and test set, while performing slightly worse in terms of MAE on both datasets. Overall, it is clear the LightGBM model is the superior model on this dataset when considering the evaluation metrics with the time taken to build and run the models. It is also important to note that neither the lightGBM and XGBoost models seemed to generalize significantly better than the other, as both of the models prediction accuracy decreased around the same values from the validation set to the testing set. An interesting point of comparison between the models is that during early stopping, the LGBM model produced an optimum value of 950 estimators, while the XGBoost model produced an optimum value of 59 estimators.\n",
    "\n",
    "In terms of feature importance, the models seem to have some agreement on the top 5 most important features for this regression task, as totals.sessionQualityDim, totals.timeOnSite, and geoNetwork.networkDomain recieved scores in the top 5 for both models feature importances. This consensus indicates these three features are crucial attributes in determining revenue per customer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
